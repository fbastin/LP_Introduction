\documentclass[usepdftitle=false]{beamer}

\usepackage[utf8]{inputenc}
\usetheme{Singapore}

\usepackage{xcolor}
% \setbeamercovered{transparent}
%\usecolortheme{crane}
\title[Notions fondamentales]{Programmation Linéaire\\Notions fondamentales}
\author[Fabian Bastin]{Fabian Bastin\\DIRO\\Université de Montréal}
\date{}

\usepackage{enumerate}
\usepackage[francais]{babel}

\usepackage{easybmat}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{tikz}

\input{macros.tex}

\setbeamertemplate{footline}[frame number]

\begin{document}
\frame{\titlepage}

% ------------------------------------------------------------------------------------------------------------------------------------------------------\begin{frame}
\begin{frame}


\titlegraphic{\vspace{-0.5cm}
	\includegraphics[scale=0.2]{imgs/udem.png}
	\hspace{2cm}
	\includegraphics[scale=0.07]{imgs/Fin-MLLogo.png}
}
\frametitle{Solutions de base}

\begin{align*}
\min_{\bsx}\ & \bsc^T \bsx \\
\mbox{s.c. } & \bsA \bsx = \bsB, \\
 & \bsx \geq 0.
\end{align*}

\mbox{}

Supposons $m \leq n$ et rang($\bsA$) = $m$.
Sans perte de généralité, supposons les $m$ premières colonnes de $\bsA$ indépendantes, et formons
\[
\bsA =
\begin{pmatrix}
 \bsB & \bsD
\end{pmatrix}
\]

\mbox{}

{\it \textcolor{blue}{Solution de base}}: $\bsx = (\bsx_b \ 0)$, avec $\bsB\bsx_b = \bsb$.\\
{\it \textcolor{blue}{Solution de base dégénérée}}: si $\bsx_b$ contient des composantes nulles. \\
{\it \textcolor{blue}{Solution de base réalisable}}: solution de base telle que $\bsA\bsx = \bsB$ et $\bsx \geq 0$.
\end{frame}

\begin{frame}
\frametitle{Rappel: base d'un espace vectoriel}

Considérons un ensemble $B = \lbrace u_1, u_2,\ldots, u_m \rbrace$ d'un sous-espace vectoriel $V$.

\begin{itemize}
\item 
Les éléments de $B$ sont \textcolor{red}{linéairement indépendants} si
$$
\sum_{i=1}^m \alpha_i u_i = 0
$$
admet pour seule solution $\alpha_i = 0$, $i = 1,\ldots,m$.
\item
$B$ est un \textcolor{red}{ensemble générateur} de $V$ si $\forall\, y \in V$, $\exists\, \alpha_i$, $i = 1,\ldots,m$ tels que
$$
\sum_{i=1}^m \alpha_i u_i = y.
$$
\item
$B$ est une \textcolor{red}{base}, si $B$ est un ensemble linéairement indépendant et générateur
de $V$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la programmation linéaire}

{\it
Soit un PL sous forme standard, avec $\bsA$ de dimension $m \times n$ et de rang plein (i.e. rang($\bsA$) = m).
\begin{itemize}
\item
S'il y a une solution réalisable, alors il y a une solution de base réalisable.
\item
S'il y a une solution réalisable optimale, alors il y a une solution de base réalisable optimale.
\end{itemize}
}

\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la programmation linéaire: preuve}

Écrivons
\[
\bsA =
\begin{pmatrix}
\vdots & \vdots & & \vdots \\
\bsa_1 & \bsa_2 & \cdots & \bsa_n \\
\vdots & \vdots & & \vdots
\end{pmatrix}
\]
et
\[
\bsx = ( \bsx_1, \bsx_2, \cdots, \bsx_n )
\]
\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la PL: réalisabilité}

Si $\bsx$ est réalisable, alors
\[
x_1 \bsa_1 + x_2 \bsa_2 + \ldots + x_n \bsa_n = \bsb.
\]
Supposons qu'il y a exactement $p$ composantes $> 0$, et s.p.d.g., qu'il s'agit des $p$ premières composantes: $\bsx_1$, \ldots, $\bsx_p$:
%. Alors, nous avons
\[
x_1 \bsa_1 + x_2 \bsa_2 + \ldots + x_p \bsa_p = \bsb.
\]

\mbox{}

\underline{Cas 1:} $\bsa_1$,\ldots,$\bsa_p$ linéairement indépendants.

Dès lors, $p \leq m$.

Si $p = m$, la preuve est complète.

Supposons donc $p < m$.
\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la PL: réalisabilité}

Comme $\bsA$ est de rang plein, on peut choisir $m-p$ vecteurs (colonnes de $\bsA$) à partir des $n-p$ vecteurs restants pour former un ensemble de $m$ vecteurs linéairement indépendants.

\mbox{}

En affectant la valeur 0 aux $m-p$ variables correspondantes, on obtient une solution de base réalisable (dégénérée). 
\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la PL: réalisabilité}

\underline{Cas 2:} $\bsa_1,\ldots,\bsa_p$ linéairement dépendants.

Dès lors, $\exists\, \bsy = (y_1, y_2,\ldots, y_p,0,\ldots,0)$ $(\in \mathcal{R}^n)$, avec au moins un $y_i > 0$, tel que
$$
y_1 \bsa_1 + y_2 \bsa_2 + \ldots y_p \bsa_p = \bszero,
$$
de sorte que $\forall \epsilon$,
$$
(x_1 - \epsilon y_1) \bsa_1 + (x_2 - \epsilon y_2) \bsa_2 + \ldots (x_p - \epsilon y_p) \bsa_p = \bsb.
$$
Autrement dit,
\[
\bsA(\bsx-\epsilon \bsy) = \bsb.
\]
Pour $\epsilon > 0$, et croissant,
\begin{itemize}
\item 
si $y_k < 0$, $(x_k - \epsilon y_k)$ augmente,
\item 
si $y_k > 0$, $(x_k - \epsilon y_k)$ diminue,
\item 
si $y_k = 0$, $(x_k - \epsilon y_k)$ reste constant.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la PL: réalisabilité}

Prenons
\begin{align*}
\epsilon &= \min_{k \in (1,2,\ldots,p)} \left\lbrace \frac{x_k}{y_k} \,\bigg|\, y_k > 0 \right\rbrace (> 0), \\
j &\in \arg \min_{k \in (1,2,\ldots,p)} \left\lbrace \frac{x_k}{y_k} \,\bigg|\, y_k > 0 \right\rbrace.
\end{align*}

Alors
$$
x_j - \epsilon y_j = 0,
$$
et $\bsx - \epsilon\bsy$ a au plus $p-1$ variables positives.

\mbox{}

En répétant ce processus si nécessaire, on peut éliminer des variables positives jusqu'à obtenir une solution réalisable avec des colonnes correspondantes qui sont linéairement indépendantes. Le cas 1 s'applique alors.

\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la PL: optimalité}

Soit $\bsx = (x_1, x_2, \ldots, x_n)$ une solution optimale réalisable, et comme précédemment, supposons qu'il y a exactement $p$ variables positives $x_1, x_2, \ldots, x_p$.

\mbox{}

À nouveau deux cas.
\begin{description}
\item[Cas 1:] correspond à l'indépendance linéaire, et se traite comme pour la question de réalisabilité.
\item[Cas 2:]
similaire à la réalisabilité, à ceci près que nous devons montrer que pour n'importe quel $\epsilon$, la solution $x-\epsilon y$ est optimale.
\end{description}

\end{frame}

\begin{frame}
\frametitle{Théorème fondamental de la PL: optimalité}

Prenons à nouveau $\bsy$ t.q. $A\bsy = 0$ et $\exists\, i \in \{1,\ldots,p\}$ t.q. $y_i > 0$.
Pour $\epsilon$ suffisamment proche de 0, positif ou négatif, $\bsx - \epsilon \bsy \geq 0$ (réalisable).

\mbox{}

Supposons $\bsc^T\bsy \ne 0$, et $\epsilon > 0$ si $\bsc^T\bsy > 0$, $\epsilon < 0$ sinon. Alors
$$
\bsc^T(\bsx - \epsilon \bsy) = \bsc^T \bsx - \epsilon \bsc^T\bsy < \bsc^T \bsx,
$$
donc $\bsx$ n'est pas optimal.
Dès lors, $\bsc^T\bsy = 0$.

\mbox{}

On peut alors appliquer le procédé du cas 2 sur la réalisabilité, pour diminuer le nombre de composantes non-nulles de la solution, tout en maintenant l'optimalité, puis se ramener au cas 1.

\end{frame}

\begin{frame}
\frametitle{Conséquences du théorème}

On peut résoudre un PL en énumérant les solutions de base réalisables.

Problème: il peut y en avoir beaucoup.

Pour $n$ variables et $m$ contraintes, nous avons
\[
\begin{pmatrix}
n \\ m
\end{pmatrix}
= \frac{n!}{m!(n-m)!}
\]
matrices à considérer pour déterminer des solutions de base.

\mbox{}

\textcolor{blue}{Exemple}: 100 variables, 40 contraintes
\[
\begin{pmatrix}
100 \\ 40
\end{pmatrix}
= 13746234145802811501267369720 \approx 1,375.10^{28}
\]

\end{frame}

\begin{frame}
\frametitle{Exemple}

Considérons le programme
\begin{align*}
\max\ & 2x + 3y \\
\mbox{t.q. } & x \geq 1 \\
& 2x + 3y \leq 5\\
& x \geq 0,\ y \geq 0.
\end{align*}

On voit immédiatement que la valeur optimale est 5 (pourquoi?).

\mbox{}

Sous forme standard, nous obtenons
\begin{align*}
- \min\ & -2x - 3y \\
\mbox{t.q. } & x - u = 1 \\
& 2x + 3y + s = 5\\
& x \geq 0,\ y \geq 0, s \geq 0, u \geq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Exemple: graphiquement}

\begin{center}
\begin{tikzpicture}[scale=1.4]
\draw[->] (0,0) -- (4,0) node[below,right] {$x$};
\draw[->] (0,0) -- (0,2) node[above,left] {$y$};

\foreach \x in {0,1,2,3}
  \draw (\x,1pt) -- (\x,-1pt) node[anchor=north] {$\x$};
\foreach \y in {0,1}
  \draw (1pt,\y) -- (-1pt,\y) node[anchor=east] {$\y$};

\filldraw[fill=yellow]
  (1,0) -- (1,1) -- (2.5,0) -- (1,0);

\draw (-0.5,2) -- (3.5,-0.6667) node[above,right] {$5=2x+3y$};
%\draw (25.0/3,-1) -- (-1,23.0/5) node[above,left] {$20=3x_1+5x_2$};
%\draw (41.0/3,-1) -- (-1,39.0/5) node[above,left] {$36=3x_1+5x_2$};

%\draw (,6) node[above] {$(2,6)$};
\fill (1,1) circle (2 pt);
\fill (2.5,0) circle (2 pt);

\end{tikzpicture}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Solutions de base}

Sous forme matricielle, le problème se définit à partir de
$$
\bsc = \begin{pmatrix}
-2 \\ -3 \\ 0 \\ 0
\end{pmatrix}, \quad
\bsx = \begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4
\end{pmatrix},\quad
\bsA = \begin{pmatrix}
1 & 0 & -1 & 0 \\
2 & 3 & 0 & 1 \\
\end{pmatrix},\quad
\bsb = \begin{pmatrix}
1 \\ 5
\end{pmatrix}
$$
Nombre de sous-matrices carrées 2 par 2:
$$
\frac{4!}{2!2!} = 6
$$
Le nombre de bases est toutefois strictement plus petit puisqu'il y a forcément des colonnes linéairement dépendantes.

\end{frame}

\begin{frame}
\frametitle{Solutions de base}

Bases potentielles:
\begin{align*}
& \bsB_1 = \begin{pmatrix} 1 & 0 \\ 2 & 3 \end{pmatrix}, \quad
\bsB_2 = \begin{pmatrix} 1 & -1  \\ 2 & 0 \end{pmatrix}, \quad
\bsB_3 = \begin{pmatrix} 1 & 0 \\ 2 & 1 \end{pmatrix}, \\
& \bsB_4 = \begin{pmatrix} 0 & -1 \\ 3 & 0 \end{pmatrix}, \quad
\bsB_5 = \begin{pmatrix} 0 & 0 \\ 3 & 1 \end{pmatrix}, \quad
\bsB_6 = \begin{pmatrix} -1 & 0 \\ 0 & 1 \end{pmatrix}.
\end{align*}

Seules $\bsB_1$, $\bsB_2$, $\bsB_3$, $\bsB_4$, $\bsB_6$ sont des bases. Les solutions de base respectives sont
$$
\begin{pmatrix}
1 \\ 1 \\ 0 \\ 0
\end{pmatrix}
\quad
\begin{pmatrix}
\frac{5}{2} \\ 0 \\ \frac{3}{2} \\ 0
\end{pmatrix}
\quad
\begin{pmatrix}
1 \\ 0 \\ 0 \\ 3
\end{pmatrix}
\quad
\begin{pmatrix}
0 \\ \frac{5}{3} \\ -1 \\ 0
\end{pmatrix}
\quad
\begin{pmatrix}
0 \\ 0 \\ -1 \\ 5
\end{pmatrix}
$$

\end{frame}

\begin{frame}
\frametitle{Solutions de base réalisables}

De ces solutions, seules les trois premières sont réalisables.

\mbox{}

Elles correspondent aux sommets du polytope réalisable.

\end{frame}

\begin{frame}
\frametitle{Relations à la convexité}

\textcolor{blue}{But:} faire le lien entre solutions de base réalisables et points extrêmes d'un polytope.

\begin{itemize}
	\item
	Un ensemble $C$ dans $E^n$ est dit {\it convexe} si pour tout $\bsx_1$, $\bsx_2 \in C$, et pour n'importe quel réel $\alpha$ tel que $0 \leq \alpha \leq 1$, le point $\alpha \bsx_1 + (1-\alpha) \bsx_2 \in C$.
	\item
Le point $z = \alpha \bsx_1 + (1-\alpha) \bsx_2$, $\alpha \in [0, 1]$, est dit être une {\it combinaison convexe} de $\bsx_1$ et $\bsx_2$.
	\item
La combinaison convexe est dite {\it stricte} si $\alpha \in (0, 1)$.
	\item
L'ensemble des combinaisons convexes de $\bsx_1$ et $\bsx_2$ est le segment de droite qui relie $\bsx_1$ et $\bsx_2$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Convexité: propriétés}

\begin{enumerate}
	\item
	Si $C$ est en ensemble convexe et $\beta$ un nombre réel, l'ensemble
	\[
	\beta C = \lbrace \bsx \,|\, \bsx = \beta \bsc,\ \bsc \in C \rbrace,
	\]
	est convexe.
	\item
	Si $C$ et $D$ sont deux ensembles convexes, l'ensemble
	\[
	C + D = \lbrace \bsx \,|\, \bsx = \bsc + \bsD,\ \bsc \in C,\ \bsD \in D \rbrace,
	\]
	est convexe.
	\item
	L'intersection de n'importe quelle collection d'ensembles convexes est convexe.
\end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Points extrêmes}

\mbox{}

\begin{itemize}
\item
{\color{red}Polytope} $P = \lbrace \bsx \,|\, \bsA \bsx \leq \bsb \rbrace$.

Note: $K = \lbrace \bsx \,|\,\ \bsA \bsx = \bsb,\ \bsx \geq 0 \rbrace$ est aussi un polytope. En effet, $K$ peut se réécrire comme
$$
K = \lbrace \bsx \,|\,\ \bsA\bsx \leq \bsb,\ -\bsA\bsx \leq -\bsb,\ -\bsx \leq 0 \rbrace.
$$
\item
{\color{red}Polyèdre}: polytope borné non vide.
\end{itemize}

\begin{itemize}
	\item 
	Voir Annexe B de Luenberger et Ye pour plus de détails.
	\item
	D'autres auteurs utilisent une convention opposée. Ainsi, dans ``Convex optimization'' (Boyd et Vandenberghe, 2004),
	un polytope est un polyèdre fermé (section 2.2.4).
\end{itemize}


%\mbox{}

%Il se peut aussi que pour $y \in C$, $\nexists\ x_1,\ x_2 \in C,\ \alpha \in (0,1)$ tel quel $y = \alpha x_1 + (1-\alpha) x_2$.
%$y$ est alors qualifié de {\color{red}point extrême}.

\end{frame}

\begin{frame}
\frametitle{Points extrêmes}

Un point $\bsx$ d'un ensemble convexe $C$ est un {\color{red}point extrême} de $C$ s'il n'existe pas deux points distincts $\bsx_1$ et $\bsx_2$ dans $C$ tels que $\bsx = \alpha \bsx_1 + (1-\alpha) \bsx_2$ pour un certain $\alpha$, $0 < \alpha < 1$.
En d'autres termes, il ne peut pas s'écrire comme une combinaison convexe stricte de deux points de $C$.
Intuitivement, un point extrême est un ``sommet'' de $C$.

%\mbox{}

%Géométriquement, cela revient à dire que chaque point du segment joignant deux points quelconques d'un ensemble convexe est aussi dans cet ensemble.

\end{frame}

\begin{frame}
\frametitle{\'Equivalence des points extrêmes et des solutions de base}

\begin{itemize}
\item
Soit $\bsA$ une matrice $m \times n$ de rang $m$ et $\bsb$ un vecteur de dimension $m$.
\item
Soit $K$ le polytope convexe constitué de l'ensemble des vecteurs $x$ de dimension $n$ satisfaisant
\begin{align*}
\bsA \bsx &= \bsb \\
\bsx & \geq 0.
\end{align*}
\end{itemize}
Un vecteur $\bsx$ est un point extrême de $K$ si et seulement si $\bsx$ est une solution de base réalisable pour le système précédent.

\end{frame}

\begin{frame}
\frametitle{Preuve}

$\Leftarrow$ Supposons tout d'abord que $\bsx = (x_1, x_2, \ldots, x_n)$ est une solution de base réalisable.
Dès lors, $\bsx$ a $k$ composantes non nulles, et $n-k$ composantes nulles, avec $k \leq m$. $k < m$ si la solution de base est dégénérée.

\mbox{}

S.p.d.g., les $k$ premières composantes sont non-nulles et
\[
  x_1 \bsa_1 + x_2 \bsa_2 + \ldots + x_k \bsa_k = \bsb,
\]
où $\bsa_1, \bsa_2, \ldots, \bsa_k$ sont les $k$ premières colonnes de $A$, linéairement indépendantes. Comme $A$ est de rang $m$, la matrice comprends $m$ colonnes indépendantes. S.p.d.g., nous pouvons supposer que les $m$ premières colonnes sont indépendantes, et on peut encore écrire
\[
x_1 \bsa_1 + x_2 \bsa_2 + \ldots + x_m \bsa_m = \bsb.
\]

\end{frame}

\begin{frame}
\frametitle{Preuve}

Supposons par l'absurde que $\bsx$ n'est pas un point extrême. Il est alors combinaison convexe stricte de deux autres points distincts de $K$:
\[
\exists \ \bsy,\ \bsz \in K,\ \bsy \ne \bsz, \ \alpha \in (0,1) \ \mbox{tels que}\ \bsx = \alpha \bsy + (1-\alpha) \bsz.
\]
Comme $\bsx \geq 0$, $\bsy \geq 0$, $\bsz \geq 0$, les $n-k$ dernières composantes de $\bsy$ et $\bsz$ sont nulles.

\mbox{}

Par définition de $K$, on a aussi
\begin{align*}
y_1 \bsa_1 + y_2 \bsa_2 + \ldots + y_m \bsa_m = \bsb, \\
z_1 \bsa_1 + z_2 \bsa_2 + \ldots + z_m \bsa_m = \bsb
\end{align*}
Comme $\bsa_1, \bsa_2, \ldots, \bsa_m$ linéairement indépendantes,
$$
\bsx = \bsy = \bsz.
$$

\end{frame}

\begin{frame}
\frametitle{Preuve}

$\Rightarrow$ Supposons à présent que $\bsx$ est un point extrême de $K$, et s.p.d.g. que les composantes non-nulles de $\bsx$ sont les $k$ premières composantes. Dès lors:
\[
x_1 \bsa_1 + x_2 \bsa_2 + \ldots + x_k \bsa_k = \bsb,
\]
avec $x_i > 0$, $i = 1,\ldots,k$.

\mbox{}

Pour montrer que $\bsx$ est une solution de base, nous devons montrer que $\bsa_1, \bsa_2,\ldots, \bsa_k$ sont linéairement indépendants. Supposons par l'absurde que ce n'est pas le cas. Alors, $\exists\, \bsy = ( y_1, y_2,\ldots,y_k,0\ldots,0) \ne 0$ t.q.
\[
y_1 \bsa_1 + y_2 \bsa_2 + \ldots + y_k \bsa_k = 0.
\]

\end{frame}

\begin{frame}
\frametitle{Preuve}

On peut prendre $\epsilon \ne 0$ suffisamment petit pour avoir
\begin{align*}
\bsx + \epsilon \bsy &\geq 0, \quad
\bsx - \epsilon \bsy \geq 0,
\end{align*}
et
\[
  \bsx = \frac{1}{2}(\bsx + \epsilon \bsy) + \frac{1}{2}(\bsx - \epsilon \bsy).
\]

\mbox{}

Clairement,
\[
\bsA(\bsx + \epsilon \bsy) = \bsA(\bsx - \epsilon \bsy) = \bsb,
\]
aussi $\bsx + \epsilon \bsy$, $\bsx - \epsilon \bsy \in K$.

\mbox{}

Dès lors, $\bsx$ peut être exprimé comme combinaison convexe de deux points distincts de $K$, et donc n'est pas un point extrême.

\mbox{}

Ceci implique qu'on doit avoir $\bsa_1,\ldots,\bsa_k$ linéairement indépendants, et de là, $k \leq m$. $\bsx$ est dès lors une solution de base.
\end{frame}

\begin{frame}
\frametitle{Corollaires}

{\bf Corollaire 1} Si l'ensemble convexe $K$ est non vide, il y a au moins un point extrême.

\mbox{}

{\bf Corollaire 2} S'il existe une solution optimale finie à un problème de programmation linéaire, il existe une solution optimale finie qui est un point extrême de l'ensemble de contraintes.

\mbox{}

{\bf Corollaire 3} L'ensemble de contraintes $K$ possède un nombre fini de points extrêmes.

{\it Preuve.} L'ensemble des points extrêmes de $K$ est un sous-ensemble des solutions de base, qui sont en nombre fini (il y a un nombre fini de sélections possible de $m$ colonnes de $\bsA$ parmi $n$ colonnes).

\mbox{}

{\bf Corollaire 4} 
Si le polytope convexe $K$ est borné, alors $K$ est un polyèdre convexe, i.e. $K$ consiste de points qui sont combinaisons convexes d'un nombre fini de points.
\end{frame}

\begin{frame}
\frametitle{Corollaire 4}

Le corollaire 4 est un cas particulier du résultat plus général:\\
\textit{(Krein-Milman) Soit $\calC$ un convexe compact de $\RR^n$. $\calC$ est l’enveloppe
convexe de ses points extrêmaux.}

\mbox{}

Note: l'enveloppe convexe d'un ensemble $\calS$ est l'intersection de tous les ensembles convexes contenant $\calS$. Si $\calS = \{ \bsx_1,\ldots,\bsx_k\}$, l'enveloppe convexe de $S$ est
$$
\rm{conv}(\calS) = \left\{ \sum_{i = 1}^k \theta_i \bsx_i \,\bigg|\, \boldsymbol{\theta} > 0, \sum_{i = 1}^k \theta_i = 1 \right\}.
$$

\end{frame}

\end{document}
