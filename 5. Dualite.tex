\documentclass[usepdftitle=false]{beamer}
\usepackage[utf8]{inputenc}

\usetheme{Singapore}
\usepackage{xcolor}

% \setbeamercovered{transparent}
%\usecolortheme{crane}
\title[Dualité]{Programmation Linéaire\\Dualité}
\author[Fabian Bastin]{Fabian Bastin\\DIRO\\Université de Montréal\\}
\date{}

\setbeamertemplate{footline}[frame number]

\usepackage[french]{babel}

\usepackage{ulem}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\def\ba{\boldsymbol{a}}
\def\bb{\boldsymbol{b}}
\def\bc{\boldsymbol{c}}
\def\be{\boldsymbol{e}}
\def\br{\boldsymbol{r}}
\def\bu{\boldsymbol{u}}
\def\bx{\boldsymbol{x}}
\def\by{\boldsymbol{y}}
\def\bz{\boldsymbol{z}}
\def\bA{\boldsymbol{A}}
\def\bB{\boldsymbol{B}}
\def\bD{\boldsymbol{D}}
\def\bH{\boldsymbol{H}}
\def\bI{\boldsymbol{I}}
\def\bL{\boldsymbol{L}}
\def\bM{\boldsymbol{M}}
\def\bU{\boldsymbol{U}}
\def\bzero{\boldsymbol{0}}
\def\blambda{\boldsymbol{\lambda}}

\def\cR{\mathcal{R}}

\def\RR{\mathbb{R}}

\begin{document}
\frame{\titlepage}

% ------------------------------------------------------------------------------------------------------------------------------------------------------\begin{frame}

\begin{frame}
\frametitle{Dualité}

Nous considérons le problème, dit {\bf primal}:
\begin{align*}
\min_x \ & c^T x \\
\mbox{t.q. } & Ax \geq b \\
& x \geq 0
\end{align*}

\mbox{}

Le programme suivant est appelé {\bf dual}:
\begin{align*}
\max_{\lambda} \ & \lambda^T b \\
\mbox{t.q. } & A^T \lambda \leq c \\
& \lambda \geq 0
\end{align*}
$A \in \RR^{m \times n}$, $c, x, \in \RR^n$, $\lambda, b \in \RR^m$.

Note: les contraintes duales peuvent aussi s'écrire $\lambda^T A \leq c^T$.
% (en appliquant l'opérateur de transposition de part et d'autre de l'inégalité).

\end{frame}

\begin{frame}
\frametitle{Dualité}

$x$: variables du problème primal\\
$\lambda$: variables du problèmes dual

\mbox{}

Dual du dual?

\mbox{}

\begin{align*}
\min_x \ & c^T x \\
\mbox{t.q. } & Ax \geq b \\
& x \geq 0
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Dualité: forme standard}

\begin{align*}
\min_x \ & c^T x \\
\mbox{t.q. } & Ax = b \\
& x \geq 0
\end{align*}

revient à

\begin{align*}
\min_x \ & c^T x \\
\mbox{t.q. } & Ax \geq b \\
& -Ax \geq -b \\
& x \geq 0
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Dualité: forme standard}

Le dual peut alors s'écrire
\begin{align*}
\max_{u, v} \ & u^T b - v^T b \\
\mbox{t.q. } & u^T A - v^T A \leq c^T \\
& u \geq 0 \\
& v \geq 0
\end{align*}
ou, avec $\lambda = u - v$,
\begin{align*}
\max_{\lambda} \ & \lambda^T b \\
\mbox{t.q. } & \lambda^T A \leq c^T \\
\end{align*}

Forme asymétrique: $\lambda \in \RR$.

\end{frame}

\begin{frame}
\frametitle{Conversion primal-dual}

\begin{center}
\begin{tabular}{|c|c|}
\hline
\hline
{\bf Minimisation} & {\bf Maximisation} \\
\hline
\hline
Contraintes & Variables \\
$\geq$ & $\geq 0$ \\
$\leq$ & $\leq 0$ \\
$=$ & non restreint \\
\hline
Variables & Contraintes \\
\hline
$\geq 0$ & $\leq$ \\
$\leq 0$ & $\geq$ \\
non restreint & $=$ \\
\hline
\end{tabular}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Conversion primal-dual: exemples}

\begin{minipage}[t]{0.49\textwidth}
\begin{center}
Primal

\mbox{}

\hrule
\begin{align*}
\min_x\ & c^Tx \\
\mbox{t.q } & Ax = b, \\
& x \geq 0. 
\end{align*}
\end{center}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
\begin{center}
Dual

\mbox{}

\hrule
\begin{align*}
\max_{\lambda}\ & b^T\lambda \\
\mbox{t.q. } & A^T\lambda \leq c. 
\end{align*}
\end{center}
\end{minipage}

\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\begin{align*}
\min_x\ & c^Tx \\
\mbox{t.q. } & Ax \geq b, \\
& x \geq 0. 
\end{align*}
\end{center}
\end{minipage}
%\hspace*{\stretch{1}}
\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\begin{align*}
\max_{\lambda}\ & b^T\lambda \\
\mbox{t.q. } & A^T\lambda \leq c, \\
& \lambda \geq 0. 
\end{align*}
\end{center}
\end{minipage}

\end{frame}

\begin{frame}
\frametitle{Conversion primal-dual: exemples}

\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\begin{align*}
\max_x\ & c^Tx \\
\mbox{t.q. } & Ax = b, \\
& x \geq 0. 
\end{align*}
\end{center}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\begin{align*}
\min_{\lambda}\ & b^T\lambda \\
\mbox{t.q. } & A^T\lambda \geq c. \\
\end{align*}
\end{center}
\end{minipage}

\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\begin{align*}
\min_x\ & c^Tx \\
\mbox{t.q } & Ax \leq b, \\
& x \geq 0. 
\end{align*}
\end{center}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
\begin{center}
\begin{align*}
\max_{\lambda}\ & b^T\lambda \\
\mbox{t.q } & A^T\lambda \leq c, \\
& \lambda \leq 0. 
\end{align*}
\end{center}
\end{minipage}

\end{frame}

\begin{frame}
	\frametitle{Exemple}
	
	Primal
	\begin{align*}
		\min_x\ & 4x_1 + 2x_2 + x_3 \\
		\text{t.q. } & x_1 + x_2 \geq 3 \\
		& -2x_1 + 2x_2 -4 x_3 \leq 5 \\
		& x_2 \geq 0,\ x_3 \leq 0.
	\end{align*}
	
	Dual
	\begin{align*}
		\max_{\lambda}\ & 3\lambda_1 + 5\lambda_2 \\
		\text{t.q. } & \lambda_1 -2\lambda_2 = 4 \\
		& \lambda_1 +2\lambda_2 \leq 2 \\
		& -4\lambda_2 \geq 1 \\
		& \lambda_1 \geq 0,\ \lambda_2 \leq 0.
	\end{align*}
	
\end{frame}

\begin{frame}
\frametitle{Exemple: le problème de régime alimentaire}

\begin{itemize}
\item
$x_j$: unités de produit alimentaire
\item
$n$ produits
\item
$b$: besoins minimums ($b_i$: $i^e$ nutriment)
\item
$c$: coût
\item
$m$ nutriments
\item
$a_{ij}$: unités de nutriments $i$ dans le produit $j$.
\end{itemize}

\textcolor{red}{Primal}: on veut minimiser sa consommation tout en satisfaisant les besoins minimums
\begin{align*}
\min_x \ & c^T x \\
\mbox{t.q. } & Ax \geq b \\
& x \geq 0
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Exemple: le problème de régime alimentaire}

\textcolor{red}{Dual}:
\begin{align*}
\max_{\lambda} \ & \lambda^T b \\
\mbox{t.q. } & \lambda^T A \leq c^T \\
& \lambda \geq 0
\end{align*}

\begin{itemize}
\item 
$\lambda$: prix de compléments alimentaires (un par nutriment).
\item
On veut maximiser le revenu de la vente de tels compléments.
\item 
Contrainte: on doit rester compétitif; le prix combiné des compléments doit rester inférieur au coût des aliments originaux.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Exemple}

Considérons le problème
\begin{align*}
\min\ &z = -4x_1 - 3x_2 -x_3 -2x_4 \\
\mbox{t.q. } & 4x_1 + 2x_2 + x_3 + x_4 \leq 5 \\
& 3x_1 + x_2 + 2x_3 + x_4 \leq 4 \\
& x_j \geq 0,\ j = 1,\ldots 6.
\end{align*}

Pour obtenir la forme standard, nous devons ajouter 2 variables d'écart, disons $x_5$ et $x_6$. Ceci donne le problème
\begin{align*}
\min\ &z = -4x_1 - 3x_2 -x_3 -2x_4 \\
\mbox{t.q. } & 4x_1 + 2x_2 + x_3 + x_4 + x_5 = 5 \\
& 3x_1 + x_2 + 2x_3 + x_4 + x_6 = 4 \\
& x_j \geq 0,\ j = 1,\ldots 6.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Exemple}

Sous forme tableau, ceci se traduit par
\[
\begin{matrix}
x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & b \\
4 & 2 & 1 & 1 & 1 & 0 & 5 \\
3 & 1 & 2 & 1 & 0 & 1 & 4 \\
-4 & -3 & -1 & -2 & 0 & 0 & 0
\end{matrix}
\]
Le système est déjà sous forme canonique, et nous pouvons identifier les variables de base $x_5$ et $x_6$. A ce système correspondent
\[
A = \begin{pmatrix}
4 & 2 & 1 & 1 & 1 & 0 \\
3 & 1 & 2 & 1 & 0 & 1 \\
\end{pmatrix},\qquad
b = \begin{pmatrix} 5 \\ 4 \end{pmatrix}
\]
Plutôt que de devoir travailler sur toutes les colonnes de $A$ en permanence, nous allons utiliser la version révisée du simplexe.

\end{frame}

\begin{frame}
\frametitle{Exemple}

Nous cherchons d'abord à calculer les coûts réduits, en notant que
\[
B = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix},\qquad 
B^{-1} = \begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix}
\]

Nous avons
\[
\lambda^T = c_B^T B^{-1} =
\begin{pmatrix} 0 & 0 \end{pmatrix}B^{-1} = \begin{pmatrix} 0 & 0 \end{pmatrix}
\]
Dès lors,
\[
r_D^T = c_D^T - \lambda^T D = c_D^T =
\begin{pmatrix} -4 & -3 & -1 & -2 \end{pmatrix}
\]
Il existe des coûts réduits négatifs, aussi nous n'avons pas terminé.

\mbox{}

Une possibilité est de faire entrer $x_1$.

\end{frame}

\begin{frame}
\frametitle{Exemple}

Dans la base courante,
\[
y_1 = B^{-1}a_1 = \begin{pmatrix} 4 \\ 3 \end{pmatrix}
\]
Le pivotage peut se résumer à
\[
\begin{matrix}
    & x_5 & x_6 & b & y_1 \\
x_5 & 1 & 0 & 5 & 4 \\
x_6 & 0 & 1 & 4 & 3 \\
-z & 0 & 0 & 0 & -4
\end{matrix}
\]
Le mininum des rapports composante par composante entre $b$ et $y_1$, pour les éléments strictement positifs de $y_1$, est 5/4. Autrement dit, $x_1$ entre dans la base et $x_5$ sort.
La réduction du tableau donne
\[
\begin{matrix}
    & x_5 & x_6 & b & y_1 \\
x_1 & 1/4 & 0 & 5/4 & 1 \\
x_6 & -3/4 & 1 & 1/4 & 0 \\
-z & 1 & 0 & 5 & 0
\end{matrix}
\]

\end{frame}

\begin{frame}
\frametitle{Exemple}

Du tableau précédent, nous tirons
\[
B^{-1} =
\begin{pmatrix}
1/4 & 0 \\
-3/4 & 1
\end{pmatrix}
\]
et en conséquence
\[
\lambda^T = \begin{pmatrix} -4 & 0 \end{pmatrix}
\begin{pmatrix}
1/4 & 0 \\
-3/4 & 1
\end{pmatrix} =  
\begin{pmatrix} -1 & 0 \end{pmatrix}
\]
Les coûts réduits deviennent
\begin{align*}
r_D^T &=
\begin{pmatrix}
-3 & -1 & -2 & 0
\end{pmatrix}
-
\begin{pmatrix}
-1 & 0
\end{pmatrix}
\begin{pmatrix}
2 & 1 & 1 & 1 \\
1 & 2 & 1 & 0
\end{pmatrix} \\
&=
\begin{pmatrix}
-3 & -1 & -2 & 0
\end{pmatrix} +
\begin{pmatrix}
2 & 1 & 1 & 1
\end{pmatrix} \\
&=
\begin{pmatrix}
-1 & 0 & -1 & 1
\end{pmatrix}
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Exemple}

Il a des coûts réduits strictement négatifs, aussi on doit continuer. On choisit ici le premier coût, autrement dit on fait entrée $x_2$, lequel est associé à
\[
y_2 = B^{-1}a_2 =
\begin{pmatrix}
1/4 & 0 \\
-3/4 & 1
\end{pmatrix}
\begin{pmatrix}
2 \\ 1
\end{pmatrix}
=
\begin{pmatrix}
1/2 \\ -1/2
\end{pmatrix}
\]
Procédons au pivotage:
\[
\begin{matrix}
    & x_5 & x_6 & b & y_2 \\
x_1 & 1/4 & 0 & 5/4 & 1/2 \\
x_6 & -3/4 & 1 & 1/4 & -1/2 \\
-z & 1 & 0 & 5 & -1
\end{matrix}
\]
\[
\begin{matrix}
    & x_5 & x_6 & b & y_2 \\
x_2 & 1/2 & 0 & 5/2 & 1 \\
x_6 & -1/2 & 1 & 3/2 & 0 \\
-z & 3/2 & 0 & 15/2 & 0
\end{matrix}
\]

\end{frame}

\begin{frame}
\frametitle{Exemple}

Nous obtenons
\[
B^{-1} =
\begin{pmatrix}
1/2 & 0 \\
-1/2 & 1
\end{pmatrix}
\]
et donc
\[
\lambda^T = \begin{pmatrix} -3 & 0 \end{pmatrix}
\begin{pmatrix}
1/2 & 0 \\
-1/2 & 1
\end{pmatrix} =  
\begin{pmatrix} -3/2 & 0 \end{pmatrix}
\]

Les coûts réduits valent
\begin{align*}
r_D^T &=
\begin{pmatrix}
-4 & -1 & -2 & 0
\end{pmatrix}
-
\begin{pmatrix}
-3/2 & 0
\end{pmatrix}
\begin{pmatrix}
4 & 1 & 1 & 1 \\
3 & 2 & 1 & 0
\end{pmatrix} \\
&=
\begin{pmatrix}
-4 & -1 & -2 & 0
\end{pmatrix} +
\begin{pmatrix}
6 & 3/2 & 3/2 & 3/2
\end{pmatrix} \\
&=
\begin{pmatrix}
2 & 1/2 & -1/2 & 3/2
\end{pmatrix}
\end{align*}
La variable d'entrée doit être $x_4$.

\end{frame}

\begin{frame}
\frametitle{Exemple}

Nous avons
\[
y_4 = 
\begin{pmatrix}
1/2 & 0 \\
-1/2 & 1
\end{pmatrix}
\begin{pmatrix}
1 \\ 1
\end{pmatrix}
=
\begin{pmatrix}
1/2 \\ 1/2
\end{pmatrix}
\]
Cela conduit au tableau
\[
\begin{matrix}
    & x_5 & x_6 & b & y_2 \\
x_2 & 1/2 & 0 & 5/2 & 1/2 \\
x_6 & -1/2 & 1 & 3/2 & 1/2 \\
-z & 3/2 & 0 & 15/2 & -1/2
\end{matrix}
\]
et $x_6$ doit sortir de la base. Le pivotage conduit à
\[
\begin{matrix}
    & x_5 & x_6 & b & y_2 \\
x_2 & 1 & -1 & 1 & 0 \\
x_4 & -1 & 2 & 3 & 1 \\
-z & 1 & 1 & 9 & 0
\end{matrix}
\]

\end{frame}

\begin{frame}
\frametitle{Exemple}

Dès lors
\[
B^{-1} =
\begin{pmatrix}
1 & -1 \\
-1 & 2
\end{pmatrix},
\]
\[
\lambda^T = \begin{pmatrix} -3 & -2 \end{pmatrix}
\begin{pmatrix}
1 & -1 \\
-1 & 2
\end{pmatrix} =  
\begin{pmatrix} -1 & -1 \end{pmatrix}
\]

Les coûts réduits sont
\begin{align*}
r_D^T &=
\begin{pmatrix}
-4 & -1 & 0 & 0
\end{pmatrix}
-
\begin{pmatrix}
-1 & -1
\end{pmatrix}
\begin{pmatrix}
4 & 1 & 1 & 0 \\
3 & 2 & 0 & 1
\end{pmatrix} \\
&=
\begin{pmatrix}
-4 & -1 & 0 & 0
\end{pmatrix} +
\begin{pmatrix}
7 & 3 & 1 & 1
\end{pmatrix} \\
&=
\begin{pmatrix}
3 & 2 & 1 & 1
\end{pmatrix}
\end{align*}
Tous les coûts réduits sont positifs. La base $B_3$ est donc optimale. La solution associée à $B_3$ est
\[
\begin{pmatrix}
0 & 1 & 0 & 3 & 0 & 0
\end{pmatrix}
\]
pour une valeur optimale de -9.

\end{frame}

\begin{frame}
\frametitle{Exemple: dual}

Reprenons le problème primal.
\begin{align*}
\min\ &z = -4x_1 - 3x_2 -x_3 -2x_4 \\
\mbox{t.q. } & 4x_1 + 2x_2 + x_3 + x_4 \leq 5 \\
& 3x_1 + x_2 + 2x_3 + x_4 \leq 4 \\
& x_j \geq 0,\ j = 1,\ldots 4.
\end{align*}
Formons le dual
\begin{align*}
\max\ & 5\lambda_1 + 4\lambda_2 \\
\mbox{t.q. } &
4\lambda_1 + 3\lambda_2 \leq -4 \\
& 2\lambda_1 + \lambda_2 \leq -3 \\
& \lambda_1 + 2\lambda_2 \leq -1 \\
& \lambda_1 + \lambda_2 \leq -2 \\
& \lambda_1 \leq 0, \lambda_2 \leq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Exemple: dual}

Essayons de nous rapprocher de la forme standard:
\begin{align*}
-\min\ & -5\lambda_1 - 4\lambda_2 \\
\mbox{t.q. } &
-4\lambda_1 - 3\lambda_2 \geq 4 \\
& -2\lambda_1 - \lambda_2 \geq 3 \\
& -\lambda_1 - 2\lambda_2 \geq 1 \\
& -\lambda_1 - \lambda_2 \geq 2 \\
& -\lambda_1 \geq 0, -\lambda_2 \geq 0.
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Exemple: dual}

En posant $y_i = -\lambda_i$, en ajoutant des variables de surplus, et en oubliant temporairement le signe négatif devant l'opérateur de minimisation, nous avons
\begin{align*}
\min\ & 5y_1 + 4y_2 \\
\mbox{t.q. } &
4y_1 + 3y_2 - y_3 = 4 \\
& 2y_1 + y_2 - y_4 = 3 \\
& y_1 + 2y_2 - y_5 = 1 \\
& y_1 + y_2 -y_6 = 2 \\
& y_1 \geq 0, y_2 \geq 0.\\
& y_3 \geq 0, y_4 \geq 0, y_5 \geq 0, y_6 \geq 0.
\end{align*}

En résolvant ce problème (avec par exemple une méthode à deux phases), nous obtenons la solution optimale
\[
y^* =
\begin{pmatrix}
1 & 1 & 0 & 0 & 0 & 0
\end{pmatrix}
\]
pour la valeur optimale 9.

\end{frame}

\begin{frame}
\frametitle{Exemple: dual}

En repassant au dual original, cela donne une valeur optimale de -9 et
\[
\lambda^* =
\begin{pmatrix}
-1 & -1
\end{pmatrix}
\]
comme lors du dernier calcul dans le simplexe révisé pour le primal.

\mbox{}

Est-ce un hasard? Pas vraiment\ldots

\end{frame}

\begin{frame}
\frametitle{Dualité faible}

(Forme symétrique ou forme asymétrique -- forme standard)

\mbox{}

Si $x$ and $\lambda$ sont réalisables pour le primal et le dual, respectivement, alors
\[
c^T x \geq \lambda^T b
\]

\begin{proof}
\[
\lambda^T b \leq \lambda^TAx \leq c^Tx,
\]
pour $x \geq 0$, vu que $x$ est supposé realisable, et que du dual, $\lambda^T A \leq c^T$.
\end{proof}

Dès lors, l'objectif primal est une borne supérieure pour le dual, et vice-versa.

\end{frame}

\begin{frame}
\frametitle{Corollaire}

Si $x_0$ et $\lambda_0$ sont réalisables pour le primal et le dual, respectivement, et si
\[
c^T x_0 = \lambda_0^T b,
\]
alors $x_0$ et $\lambda_0$ sont optimaux pour leur problème respectif.

\mbox{}

Mais on n'a encore dit sur la réalisabilité d'un problème par rapport à l'autre!

\end{frame}

\begin{frame}
\frametitle{Dualité forte}

Si un des problèmes, primal ou dual, a une solution optimale finie, l'autre problème a aussi une solution optimale finie, et les valeurs correspondantes des fonctions objectifs sont égales. Si l'un des problèmes a un objectif non borné, l'autre problème n'a pas de solution réalisable.

\begin{proof}
La deuxième affirmation est une conséquence directe du lemme.

\mbox{}

Ainsi si le primal est non borné et $\lambda$ est réalisable pour le dual, nous devons avoir
\[
\lambda^T b \leq -M
\]
pour $M$ arbitrairement grand, ce qui est impossible.

\end{proof}

\end{frame}

\begin{frame}
\frametitle{Dualité forte}

\begin{proof}
Si le primal a une solution optimale finie, nous voulons montrer que le dual a une solution optimale finie.

\mbox{}

Soit $z^*$ la valeur optimale du primal. Définissons
\begin{align*}
C = \lbrace & (r, w): r = tz^* - c^Tx,\\
& w = tb-Ax, \mbox{ avec } x \geq 0,\ t \geq 0 \rbrace
\end{align*}

\mbox{}

$C$ est un cône convexe fermé:
\begin{itemize}
\item
cône: pour $\alpha > 0$ et $(r,w) \in C$, alors $\alpha(r,w) \in C$;
\item
convexe: soient $(r_1, w_1)$ et $(r_2, w_2) \in C$, alors $\forall\, \lambda \in (0,1)$, $\lambda(r_1, w_1) + (1 -\lambda) (r_2, w_2) \in C$.
\item
fermé: $(0,0) \in C$.
\end{itemize}

\end{proof}

\end{frame}

\begin{frame}
\frametitle{Dualité forte}

\begin{proof}

Mais $(1,0) \notin C$. Par l'absurde, supposons $(1,0) \in C$.

Nous avons donc $w = 0$, et de là, il existe un certain couple $(t_0, x_0)$ tel que $t_0b - Ax_0$ = 0. Deux cas sont envisageables:
\begin{itemize}
\item
Si $t_0 > 0$, alors
$x = x_0/t_0$ est réalisable pour le primal comme $Ax = b$, $x \geq 0$. %, étant donné que $x_0 \geq 0$.
Comme $z^* \leq c^Tx$, $r = t_0z^* - t_0c^Tx_0/t_0 \leq 0$, alors qu'on supposait $r = 1$.
\item
Si $t_0 = 0$, alors $w = Ax_0 = 0$, avec $x_0 \geq 0$.
D'autre part, $1 = r = t_0z^* - c^Tx_0 = -c^Tx_0$, et donc $c^T x_0 = -1$.

Si $x$ est réalisable pour le primal, alors $\forall \alpha \geq 0$, $x+\alpha x_0$ est realisable comme $A(x+\alpha x_0) = Ax + \alpha A x_0 = b$, et $x+\alpha x_0 \geq 0$.

De plus, $c^T(x+\alpha x_0) = c^Tx - \alpha \rightarrow -\infty$ quand $\alpha \rightarrow \infty$.
Ceci contredit l'existence d'un minimum fini.
\item
Donc, $(1,0) \notin C$.
\end{itemize}

\end{proof}

\end{frame}

\begin{frame}
\frametitle{Dualité forte}

\begin{proof}

Comme $C$ est un ensemble convexe fermé, cela implique qu'il existe un hyperplan séparant $(1,0)$ et $C$. Autrement dit, $\exists [s, \lambda] \in \RR^{m+1}$, $[s, \lambda] \ne 0$, et une constante $k$ t.q.
\begin{align*}
s = (s, \lambda)^T (1,0) &< \inf \lbrace (s, \lambda)^T(r,w) \mbox{ t.q. } (r,w) \in C \rbrace \\
& = \inf \lbrace sr + \lambda^Tw \mbox{ t.q. } (r,w) \in C \rbrace = k
\end{align*}

Si $k < 0$, $\exists (r,w) \in C$ t.q. $sr + \lambda^T w = \kappa < 0$, et $\forall \alpha \geq 0$, $\alpha (r,w) \in C$ car $C$ est un cône. Mais $\alpha (s r + \lambda^T  w) = \alpha\kappa < s$, pour $\alpha$ assez grand. Donc $k \geq 0$.

\mbox{}

Or $(0,0) \in C$, donc $k \leq 0$, et de là, $k = 0$, et $s < 0$.
\end{proof}

\end{frame}

\begin{frame}
\frametitle{Dualité forte}

\begin{proof}
Prenons $\beta = -s$. Comme $C$ est un cône, nous avons
\begin{align*}
-1 = \frac{1}{\beta}s &= \frac{1}{\beta}(s,\lambda)^T(1,0) \\
&< \frac{1}{\beta}\inf \lbrace sr + \lambda^Tw \mbox{ t.q. } (r,w) \in C \rbrace \\
&= \frac{1}{\beta}\inf \lbrace s\beta r + \lambda^T \beta w \mbox{ t.q. } (r,w) \in C \rbrace \\
&= \inf \lbrace s r + \lambda^T w \mbox{ t.q. } (r,w) \in C \rbrace.
\end{align*}
Aussi, sans perte de généralités, $s = -1$

\end{proof}

\end{frame}

\begin{frame}
\frametitle{Dualité forte}

\begin{proof}
Comme $\inf \lbrace s r + \lambda^T w \mbox{ t.q. } (r,w) \in C \rbrace = 0$, si $s = -1$, $\exists \lambda \in \RR^m$ t.q.
\[
-r + \lambda^Tw \geq 0,\ \forall (r,w) \in C.
\]
De manière équivalente, par définition de $C$,
\[
(c^T-\lambda^TA)x - tz^* + t\lambda^T b \geq 0,\ \forall x, t \geq 0.
\]

\mbox{}

$t = 0$ donne $\lambda^T A \leq c^T$, i.e. $\lambda$ est réalisable pour le dual.\\
$x = 0$ et $t = 1$ donne $\lambda^T b \geq z^*$. Par le lemme 1 et son corollaire, $\lambda$ est optimal pour le dual.

\mbox{}

Comme le dual du dual est le primal, la preuve est complète.
\end{proof}

\end{frame}

\begin{frame}
\frametitle{Dualité: compatibilité}

Si un programme est non réalisable, cela n'implique cependant pas que son dual soit non borné. Celui-ci peut être non réalisable.

\mbox{}

Le tableau ci-dessous synthétise les différents cas de figure possibles.

\begin{center}
\begin{tabular}{|c||c|c|c|}
\hline
Primal / Dual & Borné & Non borné & Non réalisable \\
\hline
\hline
Borné & possible & impossible & impossible \\
\hline
Non borné & impossible & impossible & possible \\
\hline
Non réalisable & impossible & possible & possible \\
\hline
\end{tabular}
\end{center}

\end{frame}

\begin{frame}
	\frametitle{Dualité: relations à la procédure du simplexe}
	
	Résoudre le primal par le simplexe donne la solution duale.
	
	\mbox{}
	
	Supposons que le programme
	\begin{align*}
		\min_x \ & c^Tx \\
		\mbox{t.q. } & Ax = b\\
		& x \geq 0.
	\end{align*}
	a pour solution réalisable de base optimale $x = (x_{B},0)$, avec la base $B$.
	
	\mbox{}
	
	Quelle est la solution du dual
	\begin{align*}
		\max_{\lambda} \ & \lambda^T b \\
		\mbox{t.q. } & \lambda^T A \leq c^T
	\end{align*}
	en termes de $B$? Magie: on l'obtient de la résolution du primal.
	
	
\end{frame}

\begin{frame}
	\frametitle{Relations à la procédure du simplexe}
	
	Supposons
	\[
	A = \begin{pmatrix}
		B & D
	\end{pmatrix},
	\quad
	x_B = B^{-1}b,
	\quad
	r_D^T = c_D^T - c_B^T B^{-1} D.
	\]
	
	\mbox{}
	
	Si $x$ est optimal, $r_D^T \geq 0$, et donc
	\[
	c_B^T B^{-1} D \leq c_D^T.
	\]
	
	\mbox{}
	
	Avec
	\[
	\lambda^T = c_B^T B^{-1},
	\]
	nous avons
	\begin{align*}
		\lambda^T A &=
		\begin{pmatrix}
			\lambda^T B & \lambda^T D
		\end{pmatrix} \\
		&= \begin{pmatrix}
			c_B^T B^{-1}B & c_B^T B^{-1} D
		\end{pmatrix} \\
		&\leq \begin{pmatrix}
			c_B^T & c_D^T
		\end{pmatrix} = c^T.
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Relations à la procédure du simplexe}
	
	Dès lors,
	\[
	\lambda^T A \leq c^T,
	\]
	i.e. $\lambda$ est réalisable pour le dual.
	
	\mbox{}
	
	De plus,
	\[
	\lambda^T b = c_B^T B^{-1}b = c_B^T x_B
	\]
	et donc la valeur de la fonction objectif duale pour ce $\lambda$ est égale à la valeur du problème primal. Dès lors $\lambda$ est optimal pour le dual.
	
	\mbox{}
	
	On retrouve le principal résultat du théorème de dualité.
	
\end{frame}

\begin{frame}
	\frametitle{Relations à la procédure du simplexe}
	
	{\bf Théorème}
	Si le programme linéaire (sous forme standard) a une solution de base réalisable optimale, correspondant à la base $B$, le vecteur $\lambda$ t.q. $\lambda^T = c_B^T B^{-1}$ est une solution optimale du programme dual correspondant.
	Les valeurs optimales des deux programmes sont égales.
	
\end{frame}

\begin{frame}
	\frametitle{Exemple}
	
	Considérons le problème
	\begin{align*}
		\min_x \ & -x_1 - 4x_2 -3x_3 \\
		\mbox{t.q. } & 2x_1 + 2x_2 + x_3 \leq 4 \\
		& x_1 + 2x_2 + 2x_3 \leq 6 \\
		& x_1 \geq 0,\ x_2 \geq 0,\ x_3 \geq 0.
	\end{align*}
	
	\mbox{}
	
	Sous forme standard,
	\begin{align*}
		\min_x \ & -x_1 - 4x_2 -3x_3 \\
		\mbox{t.q. } & 2x_1 + 2x_2 + x_3 + x_4 = 4 \\
		& x_1 + 2x_2 + 2x_3 + x_5 = 6 \\
		& x_1 \geq 0,\ x_2 \geq 0,\ x_3 \geq 0,\ x_4 \geq 0,\ x_5 \geq 0.
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Exemple}
	
	Sous forme de tableau, cela donne
	\[
	\begin{matrix}
		2 & 2 & 1 & 1 & 0 & 4 \\
		1 & 2 & 2 & 0 & 1 & 6 \\
		-1 & -4 & -3 & 0 & 0 & 0
	\end{matrix}
	\]
	Le premier pivot donne
	\[
	\begin{matrix}
		1 & 1 & \frac{1}{2} & \frac{1}{2} & 0 & 2 \\
		-1 & 0 & 1 & -1 & 1 & 2 \\
		3 & 0 & -1 & 2 & 0 & 8
	\end{matrix}
	\]
	puis le second
	\[
	\begin{matrix}
		\frac{3}{2} & 1 & 0 & 1 & -\frac{1}{2} & 1 \\
		-1 & 0 & 1 & -1 & 1 & 2 \\
		2 & 0 & 0 & 1 & 1 & 10
	\end{matrix}
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Exemple}
	
	On a
	\[
	B = \begin{pmatrix}
		2 & 1 \\
		2 & 2
	\end{pmatrix}
	\qquad
	B^{-1} = \begin{pmatrix}
		1 & -\frac{1}{2} \\
		-1 & 1
	\end{pmatrix}
	\]
	
	\mbox{}
	
	La solution optimale est
	\[
	x_1 = 0,\ x_2 = 1,\ x_3 = 2.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Exemple: dual}
	
	\begin{align*}
		\max_{\lambda}\ & 4\lambda_1 + 6\lambda_2 \\
		\mbox{t.q. } & 2\lambda_1 + \lambda_2 \leq -1 \\
		& 2\lambda_1 + 2\lambda_2 \leq -4 \\
		& \lambda_1 + 2\lambda_2 \leq -3 \\
		& \lambda_1 \leq 0, \ \lambda_2 \leq 0.
	\end{align*}
	
	\mbox{}
	
	La solution du dual s'obtient directement de la dernière ligne du tableau du simplexe, sous les colonnes où apparaît l'identité dans le premier tableau (comme les coûts initiaux associés sont nuls):
	\[
	\lambda^T = \begin{pmatrix} -1 & -1 \end{pmatrix}.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Multiplicateurs du simplexe}
	
	\`A n'importe quelle itération du simplexe, nous pouvons former le vecteur $\lambda^T$ satisfaisant
	\[
	\lambda^T = c_B^T B^{-1}.
	\]
	
	\mbox{}
	
	Ce vecteur n'est pas une solution (réalisable) du dual à moins que $B$ ne soit une base optimale pour le primal. Mais il peut être utilisé à chaque itération pour calculer les coûts réduits, et il aura une interprétation économique.
	
	\mbox{}
	
	Pour cette raison, le vecteur $\lambda^T = c_B^T B^{-1}$ est souvent appelé le vecteur des multiplicateurs du simplexe.
	
\end{frame}

\begin{frame}
	\frametitle{Interprétation économique}
	
	Comme d'ordinaire, dénotons les colonnes de $A$ par $a_1, a_2,\ldots, a_n$, et par $e_1, e_2,\ldots, e_m$, les $m$ vecteurs unités dans $\RR^m$:
	\[
	A = \begin{pmatrix}
		\vdots & \vdots & & \vdots \\
		a_1 & a_2 & \ldots & a_m \\
		\vdots & \vdots & & \vdots
	\end{pmatrix}
	\qquad
	e_i = \begin{pmatrix}
		0 \\
		\vdots \\
		1 \\
		\vdots \\
		0
	\end{pmatrix}
	\begin{matrix}
		\\
		\\
		\rightarrow i^e \mbox{position} \\
		\\
		\\
	\end{matrix}
	\]
	
	\mbox{}
	
	\'Etant donné une base $B$, consistant de $m$ colonnes de $A$, n'importe quel autre vecteur peut être construit comme combinaison linéaire de ces vecteurs de base: soit $y \in \RR^m$,
	\[
	y = B \left(B^{-1}y\right).
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Interprétation économique}
	
	%, le coût d'un vecteur construit $y$ à partir de la base peut être calculé comme la combinaison linéaire correspondante des $c_i's$ associés à la base.

\begin{itemize}
	\item 
	L'expression de $y$ à partir de la base est donc
	\[
	B^{-1}y.
	\]
	S'il y a un coût unité $c_i$ associé avec chaque vecteur de base $a_i$, le coût associé à $y$ est
	\[
	c_B^T B^{-1}y.
	\]
	
	\item 
	Le coût du vecteur unité $e_j$ est
	$$
	c_B^T B^{-1} e_j = \lambda^T e_j = \lambda_j.
	$$

	\item 
	Dès lors, les $\lambda_j's$ peuvent être interprétés comme les prix synthétiques des vecteurs unités.
\end{itemize}	
	
\end{frame}

\begin{frame}
	\frametitle{Interprétation économique}
	
	Comme
	\[
	y = \sum_{j = 1}^m y_j e_j,
	\]
	nous avons comme coût pour $y$
	\[
	c_B^T B^{-1}y = \sum_{j = 1}^m c_B^T B^{-1} e_j y_j = \sum_{j = 1}^m \lambda_j y_j = \lambda^T y.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Interprétation économique: optimalité}
	
	L'optimalité du primal correspond à la situation où chaque vecteur $a_1, a_2,\ldots, a_n$, est ``moins cher'' quand construit à partir de la base que quand acheté directement à son propre prix.
	
	\mbox{}
	
	Dans ce cas, nous avons
	\[
	\lambda^T a_i \leq c_i,\ i = 1,2,\ldots,n,
	\]
	ou de manière équivalente
	\[
	\lambda^T A \leq c^T,
	\]
	ou encore, sous forme colonne,
	\[
	A^T\lambda \leq c,
	\]
	
\end{frame}

%\begin{frame}
%\frametitle{Faisons le point}

%Programmes primal et dual: deux points de vue d'une même réalité.

%\mbox{}

%$m$ contraintes et $n$ variables pour le primal. Simplexe sous forme révisé:
%\begin{itemize}
%\item
%taille des systèmes linéaires: $m \times m$.
%\item
%si $n$ grand par rapport à $m$: possiblement beaucoup de bases à explorer.
%\end{itemize}

%\end{frame}

\begin{frame}
	\frametitle{Sensibilité}
	
	{\it Continuation de l'interprétation des variables duales comme prix}.
	
	\mbox{}
	
	Considérons le problème standard,
	\begin{align*}
		\min_x \ & c^T x \\
		\mbox{t.q. } & Ax = b \\
		& x \geq 0
	\end{align*}
	avec la base optimale $B$ et la solution correspondante $(x_B, 0)$, où $x_B = B^{-1}b$. Une solution correspondante du dual est
	\[
	\lambda^T = c_B^T B^{-1}.
	\]
	
	\mbox{}
	
	Sous l'hypothèse de non-dégénérescence, de petits changements dans $b$ ne conduiront pas à un changement de base optimale.
	
\end{frame}

\begin{frame}
	\frametitle{Sensibilité}
	
	Considérons un petit changement $\Delta b$. Comme la base optimale n'a pas changé, la nouvelle solution optimale est
	\[
	x = (x_B + \Delta x_B, 0),
	\]
	où
	\[
	\Delta x_B = B^{-1} \Delta b.
	\]
	
	Le changement correspondant pour la fonction objectif est
	\[
	\Delta z = c_B^T \Delta x_B = \lambda^T \Delta b.
	\]
	
	\mbox{}
	
	Dès lors, $\lambda$ mesure la sensibilité de la fonction objectif à un petit changement dans le terme de droite des contraintes d'égalité: un changement de $b$ à $b+\Delta b$ conduit à un changement de la fonction objectif de $\lambda^T \Delta b$,
	
\end{frame}

\begin{frame}
	\frametitle{Sensibilité et multiplicateurs du simplexe}
	
	Puisque $\lambda_j$ est le prix du vecteur unité $e_j$ quand exprimé à partir de la base $B$, il mesure directement le changement dans le coût à partir d'un changement dans la $j^e$ composante du vecteur $b$. Cela s'observe aussi depuis la relation précédente
	\[
	\Delta z = c_B^T \Delta x_B = \lambda^T \Delta b.
	\]
	
	\mbox{}
	
	Dès lors, $\lambda_j$ peut être considéré comme le \textsl{\textcolor{red}{prix marginal}} de $b_j$, puisque modifier $b_j$ en $b_j + \Delta b_j$ conduit à un changement de la valeur optimale de $\lambda_j \Delta b_j$: si
	\[
	\Delta b = (0.\ldots,0,\Delta b_j,0,\ldots,0),
	\]
	alors
	\[
	\Delta z = \sum_{k=1}^m \lambda_k \Delta b_k = \lambda_j \Delta b_j.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Exemple: production de peinture (B. Fortz)}
	
	Une société fabrique de la peinture d'intérieur et d'extérieur à partir de deux
	produits de base $M1$ et $M2$.
	
	\mbox{}
	
	Données:
	\begin{tabular}{cccc}
		& \multicolumn{2}{c}{Quantité utilisée par tonne}
		& Quantité disponible \\
		& Extérieure & Intérieure &  par jour \\
		\hline
		M1 & 6 & 4 & 24 \\
		M2 & 1 & 2 & 6 \\
		\hline
		Profit par tonne & 5 & 4
	\end{tabular}
	
	\mbox{}
	
	Contraintes supplémentaires:
	\begin{itemize}
		\item
		Demande maximum en peinture d’intérieur: 2 tonnes par jour.
		\item
		La production en peinture d'intérieur ne peut dépasser que d’une tonne celle d'extérieur.
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Exemple: production de peinture (B. Fortz)}
	
	Variables:
	\begin{itemize}
		\item
		$x_1$ = tonnes de peinture d’extérieur produites par jour;
		\item
		$x_2$ = tonnes de peinture d’intérieur produites par jour.
	\end{itemize}
	
	\mbox{}
	
	Formulation du programme:
	\begin{align*}
		\max_x \ & z = 5x_1 + 4x_2 \\
		\mbox{s.c. } & 6x_1 + 4x_2 \leq 24 & (\lambda_1) \\
		& x_1 + 2x_2 \leq 6 & (\lambda_2) \\
		& x_2 \leq 2 & (\lambda_3) \\
		& -x_1 + x_2 \leq 1 & (\lambda_4) \\
		& x_1 \geq 0,\ x_2 \geq 0.
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Production de peinture: dual}
	
	\begin{align*}
		\min_\lambda \ & w = 24\lambda_1 + 6\lambda_2 + 2\lambda_3 + \lambda_4 \\
		\mbox{s.c. } & 6\lambda_1 + \lambda_2 - \lambda_4 \geq 5 \\
		&4\lambda_1 + 2\lambda_2 + \lambda_3 + \lambda_4 \geq 4 \\
		& \lambda_1, \lambda_2, \lambda_3, \lambda_4 \geq 0.
	\end{align*}
	
	\mbox{}
	
	Primal:
	\[
	x_1 = 3,\ x_2 = 1.5,\ z = 21
	\]
	Dual:
	\[
	\lambda_1 = 0.75,\ \lambda_2 = 0.5,\ \lambda_3 = \lambda_4 = 0,\ w = 21.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Production de peinture: interprétation économique}
	
	Interprétation de la dualité forte: le profit maximal est atteint si les ressources ont été exploitées complètement, i.e. jusqu’à épuisement de leur valeur.
	
	\mbox{}
	
	\begin{itemize}
		\item
		Le profit augmente de 0.75 par augmentation d'une tonne de M1 et de 0.5 par tonne de M2.
		\item
		Les ``ressources'' 3 et 4 sont abondantes; augmenter ces ressources n'apporte aucun profit supplémentaire.
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{\'Ecarts de complémentarité}
	
	{\bf Théorème: écarts de complémentarite -- forme asymétrique}\\
	Soit $x$ et $\lambda$ des solutions pour les programmes primal et dual, le primal étant exprimé sous forme standard. Une condition nécessaire et suffisante pour que $x$ et $\lambda$ soient tous deux solutions optimales est que pour tout $i$
	\begin{enumerate}
		\item
		$x_i > 0 \Rightarrow \lambda^T a_i = c_i$
		\item
		$x_i = 0 \Leftarrow \lambda^T a_i < c_i$
	\end{enumerate}
	
	\mbox{}
	
	\textcolor{red}{Note:} les conditions 1 et 2 peuvent se réécrire comme
	$$
	(\lambda^T a_i - c_i)x_i = 0,
	$$
	i.e. $x_i = 0$ ou (non exclusif) $\lambda^T a_i = c_i$. Sous forme vectorielle, comme $x \geq 0$ et $\lambda^T A \leq c^T$, $(\lambda^T A - c^T)x = 0$.
	
\end{frame}

\begin{frame}
	\frametitle{\'Ecarts de complémentarité: preuve du théorème}
	
	Sous les conditions énoncées,
	\[
	(\lambda^T A - c^T)x = 0.
	\]
	Dès lors,
	\[
	\lambda^T b = c^Tx,
	\]
	et par le corollaire du théorème de dualité faible, $\lambda$ et $x$ sont solutions optimales de leur problème respectif.
	
	\mbox{}
	
	De manière réciproque, si les solutions sont optimales, par le théorème de dualité forte,
	\[
	\lambda^T b = c^Tx,
	\]
	et donc
	\[
	(\lambda^T A - c^T) x = 0.
	\]
	Comme
	$x \geq 0,\quad \lambda^T A \leq c$,
	les conditions tiennent.
	
\end{frame}

\begin{frame}
	\frametitle{\'Ecarts de complémentarité}
	
	{\bf Théorème: écarts de complémentarité -- forme symétrique}\\
	Soit $x$ et $\lambda$ des solutions pour les programmes primal et dual, le primal étant exprimé avec les contraintes linéaires sous forme $Ax \geq b$. Une condition nécessaire et suffisante pour que $x$ et $\lambda$ soient tous deux solutions optimales est que pour tout $i$
	\begin{enumerate}
		\item
		$x_i > 0 \Rightarrow \lambda^T a_i = c_i$
		\item
		$x_i = 0 \Leftarrow \lambda^T a_i < c_i$
		\item
		$\lambda_j > 0 \Rightarrow a^j x = b_j$
		\item
		$\lambda_j = 0 \Leftarrow a^j x > b_j$
	\end{enumerate}
	
	\begin{proof}
		Similaire au théorème précédent.
	\end{proof}
	
\end{frame}

\begin{frame}
	\frametitle{Contraintes de complémentarité}
	
	Une \textcolor{blue}{contrainte de complémentarité} impose que deux variables sont complémentaires l'une par rapport à l'autre.
	En notant ces variables $x$ et $y$, et en les supposant de dimensions $n$, ceci se traduit par
	$$
	x_iy_i = 0,\ i = 1,\ldots,n, \quad x \ge 0, \quad y \ge 0.
	$$
	La condition ci-dessus est parfois exprimée de manière plus compacte comme
	$$
	0 \leq x \perp y \geq 0.
	$$
	
\end{frame}

\begin{frame}
	\frametitle{Contraintes de complémentarité et PL}
	
	La complémentarité traduit ici qu'une variable primale (duale) est nulle ou que la contrainte duale (primale) correspondante est active.
	
	\mbox{}
	
	Intuitivement une variable de base dans le primal correspond à une contrainte active au niveau du dual.
	
	\mbox{}
	
	Sous l'hypothèse de non-dégénérescence pour le primal et le dual, il y a exactement $m$ variables du primal non-nulles à la solution optimale et $m$ contraintes actives pour le dual.
	\begin{itemize}
		\item 
		Hypothèse de non-dégénérescence primale: $m$ composantes de la solution optimale sont strictement positives.
		\item 
		Hypothèse de non-dégénérescence duale: $m$ contraintes du dual sont actives.
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Résolution en utilisant les écarts de complémentarité}
	
	Considérons le primal
	\begin{align*}
		\max_x\ & z = 5x_1 +12x_2 +4x_3\\
		\mbox{s.c. } & x_1 + 2x_2 + x_3 \leq 10 \\
		& 2x_1 - x_2 +3x_3 = 8 \\
		& x_1, x_2, x_3 \geq 0
	\end{align*}
	Dual:
	\begin{align*}
		\min_y\ & w = 10y_1 + 8y_2 \\
		\mbox{s.c. } & y_1 + 2y_2 \geq 5 \\
		& 2y_1 - y_2 \geq 12 \\
		& y_1 + 3y_2 \geq 4\\
		& y_1 \geq 0.
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Résolution en utilisant les écarts de complémentarité}
	
	La solution optimale du primal
	\[
	\left(
	\frac{26}{5}, \frac{12}{5}, 0
	\right)
	\]
	Valeur optimale: $\frac{274}{5}$.
	
	\mbox{}
	
	Comme $x_1 > 0$, $x_2 > 0$, nous avons le système linéaire correspondant pour le dual
	\begin{align*}
		y_1 + 2y_2 = 5\\
		2y_1 - y_2 = 12.
	\end{align*}
	On tire
	\[
	5y_1 = 29,\quad y_2 = 2y_1 - 12,
	\]
	et donc
	\[
	y_1 = \frac{29}{5},\quad y_2 = -\frac{2}{5}.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Complémentarité}
	
	Considérons la paire primale-duale:
	\begin{align*}
		\min_x\ & c^T x \\
		\mbox{s.c. } & Ax = b \\
		& x \geq 0,
	\end{align*}
	et
	\begin{align*}
		\max_{\lambda} \ & b^T \lambda \\
		\mbox{s.c. } & A^T \lambda \leq c.
	\end{align*}
	
	\textit{Pour toute solution de base non dégénérée x du primal, il existe une et une seule solution $\lambda$ du dual complémentaire à $x$.}
	
	Le résultat tient même si $x$ est non-réalisable (i.e. les contraintes de non-négativité ne sont pas satisfaites).
	
\end{frame}

\begin{frame}
	\frametitle{Démonstration}
	
	Le dual peut se réécrire sous forme standard:
	\begin{align*}
		\max_{\lambda} \ & b^T\lambda \\
		\mbox{s.c. } & A^T \lambda + t = c \\
		& t \geq 0,
	\end{align*}
	ou de manière plus explicite,
	\begin{align*}
		\max_{\lambda} \ & b^T\lambda \\
		\mbox{s.c. } & a_i^T \lambda + t_i = c_i,\ i = 1,\ldots,n \\
		& t_i \geq 0,\ i = 1,\ldots,n.
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Démonstration}
	
	Considérons un ensemble de variables de base $B$ et un ensemble de variables hors base $D$, tels que la base correspondante est non dégénérée :
	\begin{align*}
		& x_B = B^{-1}b, \quad x_D = 0, \\
		& \forall\, j\ x_{Bj} \ne 0.
	\end{align*}
	
	Construisons une solution duale complémentaire à la solution primale:
	\[
	t_B = 0, \quad B^T\lambda = c_B,
	\]
	%Dès lors,
	%\[
	%\lambda^T
	%= c_B^T B^{-1},\ t_B^T = 0.
	%\]
	%De plus,
	et
	$$
	t_D^T = c_D^T - \lambda^TD = c_D^T - c_B^TB^{-1}D.
	$$
	L'unicité de $\lambda$ vient de l'inversibilité de $B^T$.
	
\end{frame}

\begin{frame}
	\frametitle{Complémentarité}
	
	On voit apparaître une réécriture des conditions de complémentarité:
	\[
	t_i x_i = 0,\ i = 1,\ldots,n.
	\]
	
	\mbox{}
	
	Problème, nous n'avons pas vérifié la condition
	$$
	t \geq 0.
	$$
	Celle-ci pourrait très bien être violée!
	
	\mbox{}
	
	Si $t \geq 0$, $c_D^T - c_B^TB^{-1}D \geq 0$, i.e. $x$ satisfait les conditions d'optimalité. Si $x \geq 0$, c'est une solution de base optimale!
	
\end{frame}

\begin{frame}
	\frametitle{Complémentarité et optimalité}
	
	Conséquence: $\lambda$ est admissible (réalisable) si et seulement si $x$ satisfait les conditions d'optimalité (coûts réduits positifs ou nuls).
	% Dans la démonstration précédente, cela revient à poser $t_B = 0$, et $t_D = (c_D^T - c_B^TB^{-1}D)$.
	Autrement dit, les conditions d'optimalité primale sont équivalentes à l'admissibilité duale.
	
	\mbox{}
	
	Similairement, une solution $x$ complémentaire à $\lambda$ est admissible si et seulement si $\lambda$ satisfait les conditions d’optimalité pour le dual. Cela se déduit directement du fait que le dual du dual est le primal.
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	Motivations:
	\begin{itemize}
		\item
		Il arrive souvent qu’une solution de base non admissible (i.e. violant les contraintes de non-négativité), mais satisfaisant les contraintes d'optimalité soit identifiables facilement (par exemple, variables d’écart de contraintes $\leq$ et composantes négatives dans le terme de droite).
		\item
		Cette base correspond à une solution admissible du dual, solution identifiable à partir des multiplicateurs du simplexe.
	\end{itemize}
	
	%Autrement dit, on peut obtenir une solution de base du problème linéaire non réalisable, mais associée à des multiplicateurs du simplexe qui sont réalisables pour le problème dual.
	
	\mbox{}
	
	Dans le tableau du simplexe, cette situation revient à ne pas avoir d'éléments négatifs dans la dernière ligne, puisque
	\begin{align*}
		r_{j} \geq 0 
		& \Leftrightarrow c_{j} - (\lambda^T D)_j \geq 0 \\
		& \Leftrightarrow c_{j} - \lambda^T a_j \geq 0 \\
		& \Leftrightarrow \lambda^T a_j \leq c_j \\
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Simplexe dual}
	
	Cette situation peut se produire tout en ayant une solution de base non réalisable.
	
	\mbox{}
	
	Ceci arrive par exemple si on résoud un problème, puis on veut en résoudre un nouveau après avoir changé $b$.
	
	\mbox{}
	
	On va alors travailler sur le problème dual en partant du tableau primal.
	
	\mbox{}
	
	Idée de la méthode simplexe duale : résoudre (implicitement) le dual par la méthode du simplexe (mais en travaillant sur le tableau primal!).
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual: principes}
	
	En termes du primal:
	\begin{itemize}
		\item
		maintenir l'optimalité de la dernière ligne;
		\item
		aller vers la réalisabilité.
	\end{itemize}
	
	\mbox{}
	
	En termes du dual:
	\begin{itemize}
		\item
		maintenir la réalisabilité;
		\item
		aller vers l'optimalité.
	\end{itemize}
	
	\mbox{}
	
	On va partir avec une solution de base satisfaisant les conditions d’optimalité (= base admissible pour le dual) et chercher à la rendre admissible (= dual optimale).
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	On considère le problème
	\begin{align*}
		\min_x \ & c^Tx \\
		\mbox{t.q. } & Ax = b\\
		& x \geq 0.
	\end{align*}
	
	Supposons qu'une base $B$ est connue, et soit
	\[
	\lambda^T = c_B^T B^{-1}.
	\]
	On suppose $\lambda$ réalisable pour le dual.
	
	\mbox{}
	
	La solution $x_b = B^{-1}b$ est dite \mbox{dual-réalisable}.
	
	\mbox{}
	
	Si $x_B \geq 0$, cette solution est aussi primal-réalisable, et est par conséquent optimale.
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	Comme $\lambda$ est réalisable pour le dual,
	\[
	\lambda^T a_j \leq c_j,\ j = 1, 2, \ldots, n.
	\]
	
	\mbox{}
	
	Si, comme d'ordinaire, nous supposons que $B$ est constitué des $m$ premières colonnes de $A$, i.e.
	\[
	B = \begin{pmatrix}
		a_1 & a_2 & \ldots & a_m
	\end{pmatrix},
	\]
	nous avons
	\[
	\lambda^T a_j = c_B^T B^{-1} a_j = c_B^T e_j = c_j,\ j = 1, 2, \ldots, m.
	\]
	où $e_j$ est le $j^e$ vecteur unité.
	
	\mbox{}
	
	En appliquant l'hypothèse de non-dégénérescence pour le dual, nous avons aussi
	\[
	\lambda^T a_j < c_j,\ j = m+1, m+2, \ldots, n.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	Un cycle du simplexe, appliqué au dual, reviendra à échanger deux composantes de $\lambda$, de manière à ce qu'une inégalité stricte devienne une égalité, et vice-versa, tout en augmentant la valeur du dual.
	
	\mbox{}
	
	Les $m$ égalités dans la nouvelle solution détermineront une nouvelle base.
	
	\mbox{}
	
	Soit $u^i$ la $i^e$ ligne de $B^{-1}$, et
	\[
	\overline{\lambda}^T = \lambda^T - \epsilon u^i.
	\]
	Nous avons (avec $\epsilon \geq 0$)
	\[
	\overline{\lambda}^T a_j = \lambda^T a_j - \epsilon u^i a_j.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	Rappelons la notation préalablement introduite
	\[
	z_j = c_B^T y_j,\qquad y_j = B^{-1}a_j.
	\]
	Dès lors,
	\[
	\lambda^T a_j = c_B^T B^{-1} a_j = z_j.
	\]
	
	\mbox{}
	
	Comme
	\begin{align*}
		u^i a_j &= y_{ij},\\
		y_j &= e_j,\ j = 1,\ldots,m
	\end{align*}
	nous avons
	\[
	u^i a_j = \delta_{ij}
	\]
	où
	\[
	\delta_{ij} =
	\begin{cases}
		1 & \mbox{si } i = j,\\
		0 & \mbox{sinon}.
	\end{cases}
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	Ainsi,
	\begin{align*}
		\overline{\lambda}^T a_j &= c_j, &\ j = 1,2,\ldots, m,\ i \ne j, \\
		\overline{\lambda}^T a_i &= c_i - \epsilon, \\
		\overline{\lambda}^T a_j &= z_j - \epsilon y_{ij}, &\ j = m+1,m+2,\ldots, n.
	\end{align*}
	
	De plus, puisque $x_B = B^{-1}b$,
	\[
	\overline{\lambda}^T b = 
	\lambda^T b - \epsilon u^i b = 
	\lambda^T b - \epsilon x_{Bi}.
	\]
	
	\mbox{}
	
	Comme
	\[
	\lambda^T a_j < c_j,\ j = m+1, \ldots, n,
	\]
	nous cherchons à augmenter le terme de gauche, ce qui revient à considérer les situations où $y_{ij} < 0$ dans
	\[
	\overline{\lambda}^T a_j = z_j - \epsilon y_{ij}, \ j = m+1,m+2,\ldots, n.
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Méthode du simplexe dual}
	
	Nous cherchons à ramener la valeur à $c_j$, sans la dépasser (sinon on violerait les conditions de réalisabilité du dual), aussi nous prenons
	\[
	\epsilon_0 = \min_j \left\lbrace \frac{z_j-c_j}{y_{ij}}\mbox{ t.q. } y_{ij} < 0 \right\rbrace.
	\]
	
	\mbox{}
	
	Soit
	\[
	k = \arg\min_j \left\lbrace \frac{z_j-c_j}{y_{ij}}\mbox{ t.q. } y_{ij} < 0 \right\rbrace.
	\]
	Nous avons
	\begin{align*}
		\overline{\lambda}^T a_k &= z_k - \epsilon_0 y_{ik} \\
		&= z_k - \frac{z_k-c_k}{y_{ik}} y_{ik} \\
		& = c_k. 
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Algorithme du simplexe dual}
	
	{\bf Etape 1}
	\'Etant donnée une solution de base dual-réalisable $x_B$, si $x_B \geq 0$, la solution est optimale: arrêt. Sinon, sélectionner un indice $i$ tel que $x_{Bi} < 0$.
	
	\mbox{}
	
	{\bf Etape 2}
	Si tous les $y_{ij} \geq 0$, $j = 1, 2,\ldots,n$, le dual n'a pas de maximum.
	Sinon, calculer
	\[
	\epsilon_0 = \min_j \left\lbrace \frac{z_j-c_j}{y_{ij}}\mbox{ t.q. } y_{ij} < 0 \right\rbrace.
	\]
	Soit $k$ l'indice correspondant (unique si l'hypothèse de non-dégénérescence s'applique).
	
	\mbox{}
	
	{\bf Etape 3}
	Former une nouvelle base $B$ en remplaçant $a_i$ par $a_k$.
	En utilisant cette base, déterminer la solution de base dual-réalisable $x_B$ correspondante.
	
\end{frame}

\begin{frame}
	\frametitle{Simplexe dual: exemple}
	
	\begin{align*}
		\min_x \ & 3x_1 + 4x_2 + 5x_3 \\
		\mbox{soumis à } & x_1 + 2x_2 + 3x_3 \geq 5 \\
		& 2x_1 + 2x_2 + x_3 \geq 6 \\
		& x_1, x_2, x_3 \geq 0.
	\end{align*}
	
	\mbox{}
	
	En introduisant des variables de surplus, nous obtenons
	\begin{align*}
		\min_x \ & 3x_1 + 4x_2 + 5x_3 \\
		\mbox{soumis à } & x_1 + 2x_2 + 3x_3 - x_4 = 5 \\
		& 2x_1 + 2x_2 + x_3 -x_5 = 6 \\
		& x_1, x_2, x_3, x_4, x_5 \geq 0.
	\end{align*}
	
\end{frame}

\begin{frame}
	\frametitle{Simplexe dual: exemple}
	
	Si nous changeons le signe des inégalités, nous obtenons
	\begin{align*}
		\min_x \ & 3x_1 + 4x_2 + 5x_3 \\
		\mbox{soumis à } & -x_1 - 2x_2 - 3x_3 + x_4 = -5 \\
		& -2x_1 - 2x_2 - x_3 + x_5 = -6 \\
		& x_1, x_2, x_3, x_4, x_5 \geq 0.
	\end{align*}
	conduisant au tableau
	\[
	\begin{matrix}
		x_4 & -1 & -2 & -3 & 1 & 0 & -5 \\
		x_5 & -2 & -2 & -1 & 0 & 1 & -6 \\
		& 3 & 4 & 5 & 0 & 0 & 0
	\end{matrix}
	\]
	La base $(a_4, a_5)$ est dual-réalisable comme tous les coûts réduits sont non-négatifs.
	
\end{frame}

\begin{frame}
	\frametitle{Simplexe dual: exemple}
	
	Nous devons sélectionner une composante de $x_B$ qui est strictement négative pour la retirer de l'ensemble des variables de base.
	Prenons par exemple $x_5 = -6$.
	
	\mbox{}
	
	Nous devons alors calculer les rapports
	\[
	\frac{z_j - c_j}{y_{2j}}
	\]
	ou, en d'autres termes, les rapports entre l'opposé des coûts réduits et les élements de la seconde ligne.
	Le plus petit rapport (strictement) positif est obtenu avec l'élément $y_{12}$:
	\[
	\begin{matrix}
		x_4 & -1 & -2 & -3 & 1 & 0 & -5 \\
		x_5 & \circled{-2} & -2 & -1 & 0 & 1 & -6 \\
		& 3 & 4 & 5 & 0 & 0 & 0
	\end{matrix}
	\]
	
\end{frame}

\begin{frame}
	\frametitle{Simplexe dual: exemple}
	
	Après le pivot, nous avons
	\[
	\begin{matrix}
		x_4 & 0 & \circled{-1} & -\frac{5}{2} & 1 & -\frac{1}{2} & -2 \\
		x_1 & 1 & 1 & \frac{1}{2} & 0 & -\frac{1}{2} & 3 \\
		& 0 & 1 & \frac{7}{2} & 0 & \frac{3}{2} & -9
	\end{matrix}
	\]
	puis
	\[
	\begin{matrix}
		x_2 & 0 & 1 & \frac{5}{2} & -1 & \frac{1}{2} & 2 \\
		x_1 & 1 & 0 & -2 & 1 & -1 & 1 \\
		& 0 & 0 & 1 & 1 & 1 & -11
	\end{matrix}
	\]
	
	\mbox{}
	
	La solution $(1, 2, 0)$ est optimale.
	
\end{frame}

\end{document}
